# ディープラーニング第5回

## 誤差逆伝搬法（バックプロパゲーション）その1

- ニューラルネットワークにデータを入力し、期待する出力の値と実際に出力された値から損失を求め、その損失が小さくなるように各ユニットの重みを更新していく手法
    - 損失(loss)とはモデルの精度の悪さを表し、損失関数から求められる
- モデルの精度を上げるために求めたいもの：損失関数の最小値を取るようなパラメータ
- しかし、ニューラルネットワークにおいて損失関数が依存するパラメータ数はとても大きい
    - 損失関数のパラメータとして各ユニットの重みなどが全て含まれるため
- そのため損失関数の最小値を取るパラメータが明示的にわからない
- そこで、最急降下法を応用して損失が小さくなるように各ユニットの重みを更新していく

### 最急降下法

- 関数の傾きを用いてその関数の最小値を探す勾配法の一種
- 対象の関数の形が複雑で解析的に厳密な最小値が求められないときに、反復計算により局所的な最適解を求めるアルゴリズム

![画像](https://github.com/kostone/dlearning/blob/master/saikyukoka.png)

$f(x)=(x-a)^2+b$の2次関数

図の2次関数の最小値を取る$x$の値が$a$であることは明らかであるが、説明のためにあえて最急降下法を利用して求める

1. ランダムに$x$の値を定める
    - $x=x_0$とすると、$x=x_0$における2次関数の接線の傾きは$\frac{d}{dx}f(x_0)$となる
    - $\frac{d}{dx}f(x_0) \gt 0$の場合、求めたい2次関数の最小値$a$は$x_0$より小さい
    - 逆に$\frac{d}{dx}f(x_0) \lt 0$の場合、最小値$a$は$x_0$よりも大きい

1. 以下のように$x$を更新していく

$$x_{new}=x_{old}-\epsilon\frac{d}{dx}f(x_{old})$$

- $\epsilon$は学習率を表す
- 繰り返し$x$を更新していくと、$x$は$a$に近づいていくことになり、そのうち求めたい答え$a$に近い値が得られる
- ただし、学習率$\epsilon$が大きすぎると$x$は収束しない
- 逆に小さすぎると収束に時間がかかる

### 活性化関数

- ニューロンに集まった入力信号の総和をどのように活性化（発火）させるかを決定するための関数
- 次の層に渡す値を整えるような役割
- 非常に種類が多い（Wikipediaでは12種類紹介されている）
- CNNでは主に非線形関数であるシグモイド関数やReLU、ソフトマックス関数、恒等関数が使われる
    - （参考）線形関数が使われない理由https://qiita.com/namitop/items/d3d5091c7d0ab669195f#%E3%81%AA%E3%81%9C%E5%A4%9A%E5%B1%A4%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%81%A7%E7%B7%9A%E5%BD%A2%E9%96%A2%E6%95%B0%E3%82%92%E4%BD%BF%E3%82%8F%E3%81%AA%E3%81%84%E3%81%AE%E3%81%8B

#### 全結合層（前回の復習）

- 一般的なニューラルネットワークでもよく使われる層
- 前層の全ユニットとその層の各ユニットが全てつながっている

$j$番目の入力を$x_j$、$i$番目の出力を$y_i$、$x_j$と$y_i$のユニット間の重み係数を$w_{ij}$、バイアスを$b_i$とすると、
$$y_i=f(\sum_jw_{ij}x_i+b_i)$$
という処理になる（$f$は活性化関数）

（復習ここまで）

- 1つのニューロンへの入力は、それぞれの入力$x_j$に重み$w_j$をかけ合わせた値にバイアス$b$を足した値となる
- この合計値に対して、特定の関数を経由することで最終的な出力値を決定する
- その出力値を決定する関数が**活性化関数**となる

下記では中間層で利用される活性化関数について紹介する（ソフトマックス関数や恒等関数は一般的に出力層で利用されるため、次回以降説明）

#### ステップ関数

$$\begin{eqnarray}
f(u)=
\begin{cases}
 x & (x \geqq 0) \\
-x & (x \lt 0)
\end{cases}
\end{eqnarray}$$

![画像](https://github.com/kostone/dlearning/blob/master/step.png)

- しきい値を境にして出力が切り替わる関数
- 一般的に、単純パーセプトロン（単層ネットワークで使われる）
- 計算結果が0より大きいか小さいかで入力値を評価する2クラスの識別問題に適していると言える
- しかし、ニューラルネットワークでは最急降下法の項目で説明したとおり関数の微分値が0でないことが重要なので、この関数は適さない

#### シグモイド関数

$$f(x)=sigmoid(x)=\frac{1}{1+e^{-x}}$$

![画像](https://github.com/kostone/dlearning/blob/master/sigmoid.png)

- 入力値が大きければ大きいほど1に近づき、小さければ小さいほど0に近づく関数
- ステップ関数と比較して、元の入力値を殺しすぎない関数と言える
- 微分しても以下のようにシグモイド関数で表すことができるという特徴を持っている
    - この特徴が誤差逆伝搬法を用いて多層ニューラルネットワークのモデルパラメータを最適化する際に意味を持つ

$$f'(x)=sigmoid(x)(1-sigmoid(x))$$

- 誤差逆伝搬法の登場以来長い間使われてきたが、以下のデメリットにより最近ではReLUを使うのが一般的になりつつある

- 出力の中心点が0.5のため、入力値が0付近だと学習時の更新量が大きくなってしまう
    - 学習の最初で重みが大きく変化してしまう
- 入力が極端に大きくなると勾配が消える
    - 学習時の更新量が非常に小さくなり、収束に時間がかかる

#### Tanh関数

$$f(x)=\tanh (x)$$

![画像](https://github.com/kostone/dlearning/blob/master/tanh.png)

- 出力範囲が-1から1の間の関数
- シグモイド関数と同様に微分しても$\tanh$関数で表すことができる

$$\frac{\partial f(x)}{\partial x}=1-f(x)^2$$

- 出力の中心点が0のため、シグモイド関数の1つ目のデメリットは解決できるが、2つ目のデメリットは解決できない

#### ReLU(Rectified Liner Unit)

$$f(x)=max(0,x)$$

![画像](https://github.com/kostone/dlearning/blob/master/relu.png)

- 入力値が正のときに入力と同じ値を、0以下のときに0を出力する関数
- 数学的には微分可能ではないが、プログラム上では微分値を定義できる
- シグモイド関数やTanh関数のデメリットを解決でき、近年のニューラルネットワークにおける活性化関数の主流となっている
- 大きなモデル、大きなデータセットを用いて学習する際に良いパフォーマンスを発揮するためには、高速に学習できることが重要
- 入力が負になったときに勾配が0になり、更新を停止するという特徴があるため、これを改善した手法もある
    - Leaky ReLU, Elu, Selu

#### 活性化関数に求められるポイント

ここまで活性化関数を見てきてわかることとして、
- 滑らかさ（勾配）がある
- 入力値が正の値は重要であるが、0以下の値は不要なので切り捨てて良い

このような特徴が求められていることがわかる。

#### 参考

https://www.hellocybernetics.tech/entry/2018/06/04/224119


### 誤差逆伝搬法（次回の内容）

説明するに当たり、下図のような単純なニューラルネットワークについて考える

画像

このニューラルネットワークの出力$k(x)$は以下のように表すことができる

$$h(x)=f(g(x))=(f \circ g)(x)$$
$$o=f(u)=sigmoid(u)=\frac{1}{1+e^{-u}}$$
$$u=g(\mathbf{x})=\mathbf{W}^T(\mathbf{x})=b$$

$f$は活性化関数、$g$は線形和を求める関数であり、今回は活性化関数$f$をシグモイド関数としている。

ニューラルネットワークのユニットは一般的に上図(a)のような線形和を求める処理と活性化を行う処理の両方を含んでいるが、今回は説明のため上図(b)のように分解している。
