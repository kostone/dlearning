# ディープラーニング第4回

## 畳み込みニューラルネットワーク(CNN)

### 畳み込みニューラルネットワークとは？

- 順伝播型の人工のディープニューラルネットワークモデルの一つ
- CNN(Convolutional Neural Network)とも呼ばれる
- ディープラーニングの中でも画像・動画認識でよく使われ、非常に高い効果を発揮しているモデル
- 畳み込み層とプーリング層を持つのが最大の特徴
- **理解するのが比較的容易（超重要）**
    - ディープラーニングを学ぶ取っ掛かりとしては最適
- 最近は自然言語処理(NLP)でも適用されることが増えている
- それ以外の分野でも使われているらしい？
    - （参考）http://pao2.hatenablog.com/entry/2018/08/04/183937
    - CNNが扱える二次元行列の形で表現できるデータであれば適用の見込みあり

### CNNの超大雑把な概要

例えばXという文字を認識したいとする。

古典的なパターンマッチングで認識を試みる場合、1ドットでも差異があるとコンピュータは「Xではない」と判定してしまう。

予め拡大縮小や回転などの前処理をする、差異の許容量を設定するなどである程度対策はできるが、それでも手書き文字の認識は難しい。
（画像）
そこでCNNでは、画像をピースに分解し、ピースごとに比較をする。

2つの画像を比較して、「ピースがほぼ一致する箇所がだいたい同じ位置関係にある」ことを検出することで、画像を認識している。
（画像）
これは画像全体のパターンマッチングよりもはるかに正確に判定できる。

そしてこのピースこそが、機械学習でおなじみの「特徴量」である。

### （復習）多層のニューラルネットワークって？

- ↑の画像は順伝搬型ニューラルネットワークと呼ばれる
- その名の通り、入力から出力に向けて一方向にデータが伝搬する
- 入力値を与える部分を入力層、出力値を得られる部分を出力層、その間にある何らかの処理を行う部分が中間層（または隠れ層）と呼ばれる
- 全部で4層以上、中間層だけで言えば2層以上あるものが「多層のニューラルネットワーク」と言える

### CNNを構成する層

ここではCNNを構成する3つの層について説明します。

#### 畳み込み層

- 畳み込み層ではさまざまなカーネルを用いて畳み込み処理を行う
    - 画像処理における畳み込み：注目画素と周辺画素の値にそれぞれ重みを付け、それらの和を出力画像の画素値にするというもの
    - カーネル：重みパラメータを保持する$n*n$の積分核。上で説明したピースと同義

- 注目する画素を一定間隔でずらしながら、入力画像全体にカーネルを適用していく
    - このような処理は画像分野ではフィルタ処理としてよく使われる
    - さまざまな種類のカーネルが存在し、カーネルの数だけ特徴マップと呼ばれるものが出力される
    - 特徴マップはカーネルと画像の一致度合いの分布を表したものと言える

（画像）
（画像）
畳み込み層と特徴マップをニューラルネットワークとして表すと次のような図になる。
（画像）

畳み込み処理の試行回数はカーネルの数（≒認識元画像の大きさ）、認識先画像の大きさ、さらに学習時には学習させる画像枚数にも比例するため、凄まじい回数となる。

ただし畳み込み処理自体は単純な加算や乗算、除算である。

このことから、前回述べたようにGPUによる演算処理が極めて有効であることがわかる。

#### プーリング層

- CNNにおいて、畳み込み層で出力された特徴マップはプーリング処理されることが多い
- プーリング処理とは？
    - **ざっくり言ってしまうと画像の縮小処理**
    - サブサンプリング処理とも呼ばれる
    - 入力画像の注目領域における平均値や最大値などを、出力される特徴マップの画素値にするというもの
        - 平均値を使うのがAvg-pooling
        - 最大値を使うのがMax-pooling
    - データサイズを減らし、対象領域の些細な差異を吸収する効果がある
    - 結果として、その領域内の特徴をロバストに取得できる

（画像）
（画像）

- 注目領域は$2*2$のサイズを使う場合が多い
- 4ドットが1ドットにプーリングされるので、データ量は単純に1/4となる

#### 全結合層

- 一般的なニューラルネットワークでもよく使われる層
- 前層の全ユニットとその層の各ユニットが全てつながっている

$j$番目の入力を$x_j$、$i$番目の出力を$y_i$、$x_j$と$y_i$のユニット間の重み係数を$w_{ij}$、バイアスを$b_i$とすると、
$$y_i=f(\sum_jw_{ij}x_i+b_i)$$
という処理になる。（$f$は活性化関数、次回説明）

#### 引用元リンク
http://brohrer.github.io/how_convolutional_neural_networks_work.html
