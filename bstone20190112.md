# ディープラーニング第6回

## 誤差逆伝播法（バックプロパゲーション）その2

### 誤差逆伝播法

説明するに当たり、下図(a)のような単純なニューラルネットワークについて考える。

画像

このニューラルネットワークの出力$h(x)$は以下のように表すことができる。

$$h(x)=f(g(x))=(f \circ g)(x)$$
$$o=f(u)=sigmoid(u)=\frac{1}{1+e^{-u}}$$
$$u=g(\mathbf{x})=\mathbf{W}^T(\mathbf{x})=b$$

$f$は活性化関数、$g$は線形和を求める関数であり、今回は活性化関数$f$をシグモイド関数としている。

ニューラルネットワークのユニットは一般的に上図(a)のような線形和を求める処理と活性化を行う処理の両方を含んでいるが、今回は説明のため上図(b)のように分解している。

損失関数を以下のような二乗和誤差関数とし、入力$\mathbf{x}=(x_1,…,x_n)$に対し、教師信号$t$が与えられているとき、損失は以下のように算出される。

$$E=\frac{1}{2}\|t-h(\mathbf{x})\|^2$$

ここで、$E$を活性化後の値$o$で偏微分した値は以下のようになり、誤差信号$\delta^o$と定義する。

$$\delta^o\equiv\frac{\partial E}{\partial o}=-(t-o)$$

今回は単純なニューラルネットワークであるため、活性化関数の出力$o$がそのまま$h(x)$になっていることに注意する。また、$E$を線形和の値$u$で偏微分した値は以下のようになり、誤差信号$\delta^u$と定義する。

$$\delta^u\equiv\frac{\partial E}{\partial u}=\frac{\partial E}{\partial o}\frac{\partial o}{\partial u}=\delta^oo(1-o)$$

以上より、重み$W$とバイアス$b$の修正量は合成関数の微分における連鎖率を用いて以下のように求めることができる。

$$\Delta\mathbf{W}=\frac{\partial E}{\partial \mathbf{W}}=\frac{\partial E}{\partial u}\frac{\partial u}{\partial \mathbf{W}}=\delta^u\mathbf{x}$$

$$\Delta b=\frac{\partial E}{\partial b}=\frac{\partial E}{\partial u}\frac{\partial u}{\partial b}=\delta^u$$

$W$と$b$は以下の式のように最急降下法を適用して更新する。

$$\mathbf{W}_{new}=\mathbf{W}_{old}-\epsilon\Delta\mathbf{W}$$

$$b_{new}=b_{old}-\epsilon\Delta b$$


ここまでは単純なニューラルネットワークを前提に重み$W$とバイアス$b$を更新することを考えてきたが、ここからは下図の部分を持つような多層のニューラルネットワークについて考える。

図

このとき、この多層ニューラルネットワークの入力と出力は以下のようになる。

$$\mathbf{o}^1=\mathbf{x}$$

$$o^{k_{max}}=h-(\mathbf{x})$$

便宜上、ここでも全ての活性化関数をシグモイド関数とすると、$o$を$u$で偏微分した値は以下のように表される。

$$\frac{\partial o^{k+1}}{\partial u^{k+1}}=o^{k+1}(1-o^{k+1})$$

その他、$u$を各変数で偏微分した値を以下に示す。

$$\frac{\partial u^{k+1}}{\partial o^k}=\mathbf{W}^k$$

$$\frac{\partial u^{k+1}}{\partial \mathbf{W}^k}=\mathbf{o}^k$$

$$\frac{\partial u^{k+1}}{\partial b^k}=1$$

以上より、活性化処理を行うそうにおける誤差信号$\delta^{o^k}$は以下のように表すことが可能となる。

$$\delta^{o^k}=\frac{\partial E}{\partial \mathbf{o}^k}=\begin{cases}-(t-\mathbf{o}^k)& if k=k_{max}\\\frac{\partial E}{\partial \mathbf{u}^{k+1}}\frac{\partial \mathbf{u}^{k+1}}{\partial \mathbf{o}}=\delta^{u^{k+1}}\mathbf{W}^k& otherwise\end{cases}$$

このとき、損失関数は先程と同じように二乗和誤差関数としている。
したがって、線形和層における重み$W$とバイアス$b$の修正量は、単純なニューラルネットワークのときと同じく、連鎖律を用いて以下のように求めることができる。

$$\frac{\partial E}{\partial \mathbf{W}^k}=\frac{\partial E}{\partial u^{k+1}}\frac{\partial u^{k+1}}{\partial \mathbf{W}^k}=\delta^{u^{k+1}}\mathbf{o^k}$$

$$\frac{\partial E}{\partial b^k}=\frac{\partial E}{\partial u^{k+1}}\frac{\partial u^{k+1}}{\partial b^k}=\delta^{u^{k+1}}$$

以上のように、出力層の方から順番に1つ前の層のパラメータを更新していくアルゴリズムが誤差逆伝播法である。

- 重みの初期値は毎回乱数で初期化する。そのため、同じ設定かつ同じデータを用いたとしても、学習するたびに異なるモデルになる。
- ここでは全結合層を例に誤差逆伝播法について説明したが、畳み込み層を用いても変わりはない。
  - 畳み込み層はニューラルネットワークとして下図のように表現されるので、ユニット間の接続がない場所の重みは常に0と考えて誤差逆伝播法を適用すればよいだけである。

### 活性化関数（出力層で使用）

前回紹介できなかった出力層で使われる活性化関数を2つ紹介

#### ソフトマックス関数

$$f_i(a)=\frac{e^{a_i}}{\sum_j^n e^{a_j}} \ for \ i=1,…,n$$

画像

- シグモイド関数を多変量に適応させた関数
- $f_i(a)$は$(0,1)$の範囲を取る
- $\sum_{i=1}^n f_i(a)=1$であるため、他クラス分類などにおける離散確率分布としても扱うことができる
- CNNを含むニューラルネットワークにおいては、ある入力が各クラスに属する確率を算出するために出力層で利用されることが多い
  - （例）果物の写真の3クラス分類問題において、出力層でソフトマックス関数を適用したら、みかん：0.8、りんご：0.2、もも：0という結果が得られた
- シグモイド関数と同様に、ソフトマックス関数の$a_j$に関する偏微分も以下のようにソフトマックス関数自身で表すことができる

$$\frac{\partial f_i}{\partial a_j}=f_i(\delta_{ij}-f_j)$$
※ $\delta_{ij}$はクロネッカーのデルタ

##### （参考）シグモイド関数

$$f(x)=sigmoid(x)=\frac{1}{1+e^{-x}}$$
$$f'(x)=sigmoid(x)(1-sigmoid(x))$$

#### 恒等関数

$$f(x)=x$$

画像

- 入力した値と同じ値を常にそのまま返す関数
- 一般的に、回帰問題のような確率に変換する必要がない場合に使われる
- （「活性化関数として恒等関数を使う」くらいなら「活性化関数を使わない」方が簡単なのでは？）
- https://detail.chiebukuro.yahoo.co.jp/qa/question_detail/q10167160965


### 次回のネタ（自分用メモ）

- 代表的なCNNアーキテクチャの紹介
- よく使われるデータセットの紹介