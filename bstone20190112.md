# ディープラーニング第6回

## 誤差逆伝播法（バックプロパゲーション）その2

### 誤差逆伝播法

説明するに当たり、下図(a)のような単純なニューラルネットワークについて考える。

画像

このニューラルネットワークの出力$h(x)$は以下のように表すことができる。

$$h(x)=f(g(x))=(f \circ g)(x)$$
$$o=f(u)=sigmoid(u)=\frac{1}{1+e^{-u}}$$
$$u=g(\mathbf{x})=\mathbf{W}^T(\mathbf{x})=b$$

$f$は活性化関数、$g$は線形和を求める関数であり、今回は活性化関数$f$をシグモイド関数としている。

ニューラルネットワークのユニットは一般的に上図(a)のような線形和を求める処理と活性化を行う処理の両方を含んでいるが、今回は説明のため上図(b)のように分解している。

損失関数を以下のような二乗和誤差関数とし、入力$\mathbf{x}=(x_1,…,x_n)$に対し、教師信号$t$が与えられているとき、損失は以下のように算出される。

$$E=\frac{1}{2}\|t-h(\mathbf{x})\|^2$$

ここで、$E$を活性化後の値$o$で偏微分した値は以下のようになり、誤差信号$\delta^o$と定義する。

$$\delta^o\equiv\frac{\partial E}{\partial o}=-(t-o)$$

今回は単純なニューラルネットワークであるため、活性化関数の出力$o$がそのまま$h(x)$になっていることに注意する。また、$E$を線形和の値$u$で偏微分した値は以下のようになり、誤差信号$\delta^u$と定義する。

$$\delta^u\equiv\frac{\partial E}{\partial u}=\frac{\partial E}{\partial o}\frac{\partial o}{\partial u}=\delta^oo(1-o)$$

以上より、重み$W$とバイアス$b$の修正量は合成関数の微分における連鎖率を用いて以下のように求めることができる。

$$\Delta\mathbf{W}=\frac{\partial E}{\partial \mathbf{W}}=\frac{\partial E}{\partial u}\frac{\partial u}{\partial \mathbf{W}}=\delta^u\mathbf{x}$$

$$\Delta b=\frac{\partial E}{\partial b}=\frac{\partial E}{\partial u}\frac{\partial u}{\partial b}=\delta^u$$

$W$と$b$は以下の式のように最急降下法を適用して更新する。

$$\mathbf{W}_{new}=\mathbf{W}_{old}-\epsilon\Delta\mathbf{W}$$
$$b_{new}=b_{old}-\epsilon\Delta b$$


ここまでは単純なニューラルネットワークを前提に重み$W$とバイアス$b$を更新することを考えてきたが、ここからは下図の部分を持つような多層のニューラルネットワークについて考える。

図

このとき、この多層ニューラルネットワークの入力と出力は以下のようになる。

$$\mathbf{o}^1=\mathbf{x}$$

$$o^{k_{max}}=h-(\mathbf{x})$$

便宜上、ここでも全ての活性化関数をシグモイド関数とすると、$o$を$u$で偏微分した値は以下のように表される。

$$\frac{\partial o^{k+1}}{\partial u^{k+1}}=o^{k+1}(1-o^{k+1})$$

その他、$u$を各変数で偏微分した値を以下に示す。

$$\frac{\partial u^{k+1}}{\partial o^k}=\mathbf{W}^k$$

$$\frac{\partial u^{k+1}}{\partial \mathbf{W}^k}=\mathbf{o}^k$$

$$\frac{\partial u^{k+1}}{\partial b^k}=1$$

以上より、活性化処理を行うそうにおける誤差信号$\delta^{o^k}$は以下のように表すことが可能となる。

$$\delta^{o^k}=\frac{\partial E}{\partial \mathbf{o}^k}=\begin{cases}-(t-\mathbf{o}^k)& if k=k_{max}\\\frac{\partial E}{\partial \mathbf{u}^{k+1}}\frac{\partial \mathbf{u}^{k+1}}{\partial \mathbf{o}}=\delta^{u^{k+1}}\mathbf{W}^k& otherwise\end{cases}$$

このとき、損失関数は先程と同じように二乗和誤差関数としている。
したがって、線形和層における重み$W$とバイアス$b$の修正量は、単純なニューラルネットワークのときと同じく、連鎖律を用いて以下のように求めることができる。

$$\frac{\partial E}{\partial \mathbf{W}^k}=\frac{\partial E}{\partial u^{k+1}}\frac{\partial u^{k+1}}{\partial \mathbf{W}^k}=\delta^{u^{k+1}}\mathbf{o^k}$$

$$\frac{\partial E}{\partial b^k}=\frac{\partial E}{\partial u^{k+1}}\frac{\partial u^{k+1}}{\partial b^k}=\delta^{u^{k+1}}$$

以上のように、出力層の方から順番に1つ前の層のパラメータを更新していくアルゴリズムが誤差逆伝播法である。

- 重みの初期値は毎回乱数で初期化する。そのため、同じ設定かつ同じデータを用いたとしても、学習するたびに異なるモデルになる。
- ここでは全結合層を例に誤差逆伝播法について説明したが、畳み込み層を用いても変わりはない。
  - 畳み込み層はニューラルネットワークとして下図のように表現されるので、ユニット間の接続がない場所の重みは常に0と考えて誤差逆伝播法を適用すればよいだけである。

### 活性化関数（出力層）

#### ソフトマックス関数